{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Question and Answering\n",
    "\n",
    "This notebook is about Visual Question and answering.\n",
    "To visualize the tensorboard tensorboard --logdir=boards/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some changes that need to be done\n",
    "1. Change the basic architecture to the attention architecture.\n",
    "2. Visualize the tensorboard properly to keep it in the final report. \n",
    "3. Model design.\n",
    "    1. Change the final outputs. I multiplied the question embedding with the answer embedding. This has to be chnaged. \n",
    "    2. Can directly take the fc7 outputs. \n",
    "    3. Can pass the image as the first token. -> This should not make much difference. \n",
    "    4. Activations somewhere in the network. \n",
    "    5. Change the random sampling to get the vaidation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import string\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from helpers.config import *\n",
    "from helpers.preprocessing import *\n",
    "from helpers.utils_v2 import *\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resetting default tensorflow computational Graph\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_Size:  28\n",
      "num_epochs:  25\n",
      "Weights file is :  ../weights/vgg16_weights.npz\n",
      "Config data path is:  ../data/dataset_v7w_telling.json\n",
      "Glove vectors path is:  ../data/glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "cfg = Config(batch_size=28, num_epochs=25)\n",
    "print(\"Batch_Size: \", cfg.batch_size)\n",
    "print(\"num_epochs: \", cfg.num_epochs)\n",
    "print(\"Weights file is : \", cfg.weights_path)\n",
    "print(\"Config data path is: \", cfg.data_path)\n",
    "print(\"Glove vectors path is: \", cfg.glove_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data required for the Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples are:  111894\n",
      "Validation examples number:  27974\n"
     ]
    }
   ],
   "source": [
    "samples = loadData(cfg.data_path.split('../')[1])\n",
    "train_samples, val_samples = train_test_split(samples, test_size=0.2)\n",
    "print(\"Total number of training samples are: \", len(train_samples))\n",
    "print(\"Validation examples number: \", len(val_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading the Glove vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "## Loading glove vectors here. \n",
    "W2VEC = load_glove(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions available in the utils.py\n",
    "\n",
    "1. Encoding the image\n",
    "2. Encoding the text\n",
    "3. Loading the Weights of the pretrained model\n",
    "4. Loading placeholders\n",
    "5. Variables class\n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "1. Load the image, question and answer here and train the network. \n",
    "2. Will get the output data in the shape (N, image, question, answer, groundtruth, option1, option2, option3)\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2VEC_LEN = 50\n",
    "\n",
    "def vectorize(words_sequence, max_words=15, clean=False):\n",
    "    'Takes a sentence and returns corresponding list of GloVecs'\n",
    "\n",
    "    if clean:\n",
    "        sent = _dataCleaning(words_sequence)\n",
    "\n",
    "    words = words_sequence.lower().translate(string.punctuation).strip().split()\n",
    "    # ignoring words beyond max_words\n",
    "    words = words[:max_words]\n",
    "    words2vec = np.empty((1, W2VEC_LEN))\n",
    "\n",
    "    for w in words:\n",
    "        word2vec = W2VEC.get(w.lower())\n",
    "\n",
    "        if word2vec is None:\n",
    "            word2vec = np.random.rand(W2VEC_LEN)\n",
    "\n",
    "        word2vec = word2vec.reshape((1, W2VEC_LEN))\n",
    "        words2vec = np.concatenate((words2vec, word2vec), axis=0)\n",
    "\n",
    "    PADDING = np.zeros((1, W2VEC_LEN))\n",
    "\n",
    "    for _ in np.arange(max_words - len(words)):\n",
    "        words2vec = np.concatenate((words2vec, PADDING), axis=0)\n",
    "\n",
    "    return words2vec[1:]\n",
    "\n",
    "def generator(train_samples, batch_size=32):\n",
    "    \n",
    "    \"\"\"\n",
    "    1. Reads the image\n",
    "    2. Reads the question and appends the word2vec for the sentence. \n",
    "    3. Reads the answer and the options and appends the word2vec to the corresponding lists. \n",
    "    4. Have to tokenize the question and answer here. \n",
    "    \n",
    "    May need preprocessing of the question here. Get word 2 vecs of the word here. \n",
    "    The shape of the questions, answers, options1 to options3 is (N, T, D)\n",
    "    \n",
    "        N - number of samples\n",
    "        T - time steps in the RNN\n",
    "        D - dimension of the word 2 vector\n",
    "        \n",
    "    Returns: 1. Images batch, \n",
    "             2. Questions batch ,\n",
    "             3. Answers batch, \n",
    "             4. option1 batch, \n",
    "             5. option2 batch, \n",
    "             6. option3 batch\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples = len(train_samples)\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        sklearn.utils.shuffle(train_samples)\n",
    "        \n",
    "        path_to_images = \"images/\"\n",
    "        \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            batch_samples = train_samples[offset:offset+batch_size]\n",
    "            \n",
    "            train_images = []\n",
    "            questions = []\n",
    "            answers = []\n",
    "            options1 = []\n",
    "            options2 = []\n",
    "            options3 = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                image_path = batch_sample[0]\n",
    "                question   = batch_sample[1]\n",
    "                answer     = batch_sample[2]\n",
    "                choice1    = batch_sample[3]\n",
    "                choice2    = batch_sample[4]\n",
    "                choice3    = batch_sample[5]\n",
    "                \n",
    "                image1 = cv2.imread( path_to_images + batch_sample[0] )\n",
    "                image1 = cv2.resize(image1, (448,448))\n",
    "                train_images.append(image1)\n",
    "                \n",
    "                questions.append(vectorize(question, max_words = cfg.question_max_words))\n",
    "                \n",
    "                answers.append(vectorize(answer, max_words = cfg.answer_max_words))\n",
    "                options1.append(vectorize(choice1, max_words = cfg.answer_max_words))\n",
    "                options2.append(vectorize(choice2, max_words = cfg.answer_max_words))\n",
    "                options3.append(vectorize(choice3, max_words = cfg.answer_max_words))\n",
    "                \n",
    "            \n",
    "            train_images = np.array(train_images)\n",
    "            questions = np.array(questions)\n",
    "            answers = np.array(answers)\n",
    "            options1 = np.array(options1)\n",
    "            options2 = np.array(options2)\n",
    "            options3 = np.array(options3)\n",
    "            \n",
    "            labels = np.zeros([batch_size,4])\n",
    "            labels[:,0] = 1\n",
    "            yield train_images, questions, answers, options1, options2, options3, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels_placeholder):\n",
    "    \"\"\"\n",
    "    Considering that the score is the final logit value without the softmax. \n",
    "    \"\"\"\n",
    "    final_loss = tf.reduce_sum(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_placeholder, dim=-1))\n",
    "    return final_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights path is:  ../weights/vgg16_weights.npz\n",
      "The weights are trainable\n",
      "weight_file is:  weights/vgg16_weights.npz\n"
     ]
    }
   ],
   "source": [
    "## 1\n",
    "inputIm_placeholder, question_placeholder, answer_placeholder, \\\n",
    "option1_placeholder, option2_placeholder, option3_placeholder, labels_placeholder = load_placeholders(cfg)\n",
    "\n",
    "## 2. \n",
    "encode_image = encodeImage(cfg)\n",
    "\n",
    "## 3. \n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(\"Weights path is: \", cfg.weights_path)\n",
    "\n",
    "## 4. \n",
    "with sess.as_default():\n",
    "    encode_image.load_weights(cfg.weights_path, sess, is_Train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Before) final_conv_layer shape:  (?, 8, 8, 512)\n",
      "(After) final_conv_layer shape:  (?, 32768)\n",
      "output_fully_connected shape is:  (?, 512)\n",
      "pro_value1 shape is:  (?,)\n",
      "pro_value shape is:  (?, 4)\n",
      "Accuracy_operatin shape is:  (?,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Loading Placeholders for the computational graph\n",
    "2. Creating object for the encoding image and encoding text\n",
    "3. Creating default session in tensorflow\n",
    "4. As the CNN model is pre trained, the loads are loaded in the encoder object within the defautl session. \n",
    "5. The computational graph is run for the convolution part. \n",
    "6. Encoding the question using the computational graph. \n",
    "7. \n",
    "\"\"\"\n",
    "\n",
    "## 5. \n",
    "final_conv_layer = encode_image.forward_pass(inputIm_placeholder)\n",
    "print(\"(Before) final_conv_layer shape: \", final_conv_layer.get_shape())\n",
    "\n",
    "## Need to flatten the image here. \n",
    "final_conv_layer = tf.contrib.layers.flatten(final_conv_layer)\n",
    "print(\"(After) final_conv_layer shape: \", final_conv_layer.get_shape())\n",
    "\n",
    "fully_connected_object = fullyConnected(cfg)\n",
    "output_fully_connected = fully_connected_object.forward_pass(final_conv_layer)\n",
    "print(\"output_fully_connected shape is: \", output_fully_connected.get_shape())\n",
    "\n",
    "# init_state = tf.Variable(tf.zeros([cfg.batch_size, cfg.state_size], dtype = tf.float32))\n",
    "\n",
    "## 6. \n",
    "with tf.variable_scope(\"question\", reuse=tf.AUTO_REUSE):\n",
    "    \"\"\"\n",
    "    Reuse permission is given to all the variables within this module. \n",
    "    \"\"\"\n",
    "    encode_text = encodeText(cfg)\n",
    "    output_fw_q, final_state_fw_q = encode_text.encode(question_placeholder, encoder_input=output_fully_connected)\n",
    "\n",
    "with tf.variable_scope(\"answers\", reuse=tf.AUTO_REUSE):\n",
    "    \"\"\"\n",
    "    Reuse Permission is given to the answer as well.\n",
    "    \"\"\"\n",
    "    encode_answer = encodeText(cfg)\n",
    "    output_fw_a, final_state_fw_a = encode_answer.encode(answer_placeholder, final_state_fw_q)\n",
    "    output_fw_opt1, final_state_fw_opt1 = encode_answer.encode(option1_placeholder, final_state_fw_q)\n",
    "    output_fw_opt2, final_state_fw_opt2 = encode_answer.encode(option2_placeholder, final_state_fw_q)\n",
    "    output_fw_opt3, final_state_fw_opt3 = encode_answer.encode(option3_placeholder, final_state_fw_q)\n",
    "\n",
    "\"\"\"\n",
    "Now I have to do the dot product of the two outputs and then send it to the loss function. \n",
    "\"\"\"\n",
    "pro_value1 = tf.reduce_sum(tf.multiply(final_state_fw_q, final_state_fw_a), axis=1)\n",
    "pro_value2 = tf.reduce_sum(tf.multiply(final_state_fw_q, final_state_fw_opt1), axis=1)\n",
    "pro_value3 = tf.reduce_sum(tf.multiply(final_state_fw_q, final_state_fw_opt2), axis=1)\n",
    "pro_value4 = tf.reduce_sum(tf.multiply(final_state_fw_q, final_state_fw_opt3), axis=1)\n",
    "\n",
    "print(\"pro_value1 shape is: \", pro_value1.get_shape())\n",
    "\n",
    "pro_value = tf.stack([pro_value1, pro_value2, pro_value3, pro_value4], axis=1)\n",
    "print(\"pro_value shape is: \", pro_value.get_shape())\n",
    "loss = compute_loss(pro_value, labels_placeholder)\n",
    "\n",
    "accuracy_operation = tf.argmax(pro_value, axis=1)\n",
    "print(\"Accuracy_operatin shape is: \", accuracy_operation.get_shape())\n",
    "non_zero_count = tf.count_nonzero(accuracy_operation)\n",
    "accuracy = cfg.batch_size - non_zero_count\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=3e-4).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [25088,4096] and type float\n\t [[Node: zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [25088,4096] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'zeros_13', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-40cd511e1761>\", line 13, in <module>\n    encode_image.load_weights(cfg.weights_path, sess, is_Train=True)\n  File \"/home/svh2811/VisualQA/helpers/utils_v2.py\", line 116, in load_weights\n    keys, weights = load_weights(self.cfg.weights_path, sess, is_Train)\n  File \"/home/svh2811/VisualQA/helpers/utils_v2.py\", line 71, in load_weights\n    'fc6_W': tf.Variable(tf.zeros([25088, 4096], dtype=tf.float32), name='fc6_W', trainable=is_Train),\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1512, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 218, in constant\n    name=name).outputs[0]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3076, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1561, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [25088,4096] and type float\n\t [[Node: zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [25088,4096] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [25088,4096] and type float\n\t [[Node: zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [25088,4096] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3bbf7fd2ca0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [25088,4096] and type float\n\t [[Node: zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [25088,4096] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'zeros_13', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-40cd511e1761>\", line 13, in <module>\n    encode_image.load_weights(cfg.weights_path, sess, is_Train=True)\n  File \"/home/svh2811/VisualQA/helpers/utils_v2.py\", line 116, in load_weights\n    keys, weights = load_weights(self.cfg.weights_path, sess, is_Train)\n  File \"/home/svh2811/VisualQA/helpers/utils_v2.py\", line 71, in load_weights\n    'fc6_W': tf.Variable(tf.zeros([25088, 4096], dtype=tf.float32), name='fc6_W', trainable=is_Train),\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1512, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 218, in constant\n    name=name).outputs[0]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3076, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1561, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [25088,4096] and type float\n\t [[Node: zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [25088,4096] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "## Define the Classifier. \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "savefile = \"models/model1.ckpt\"\n",
    "\n",
    "with sess.as_default():\n",
    "    writer = tf.summary.FileWriter(\"boards/1\")\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(cfg.num_epochs):\n",
    "\n",
    "        print(\"Epoch Number: \", i)\n",
    "        batch_generator = generator(train_samples, cfg.batch_size)\n",
    "        total_iterations = int(len(train_samples) / cfg.batch_size)\n",
    "\n",
    "        for j in range(total_iterations):\n",
    "            #if(j%10==0):\n",
    "            #print ( \"Iter: \",j)\n",
    "            start_time = time.time()\n",
    "            batch_images_gen, batch_questions_gen, batch_answers_gen, batch_o1, batch_o2, batch_o3, labels = batch_generator.__next__()\n",
    "\n",
    "            sess.run(train_step, feed_dict= \\\n",
    "                {inputIm_placeholder: batch_images_gen, \\\n",
    "                 question_placeholder: batch_questions_gen, \\\n",
    "                 answer_placeholder: batch_answers_gen, \\\n",
    "                 option1_placeholder: batch_o1, \\\n",
    "                 option2_placeholder: batch_o2, \\\n",
    "                 option3_placeholder: batch_o3, \\\n",
    "                 labels_placeholder: labels\n",
    "                 })\n",
    "\n",
    "            if (j % 50 == 0):\n",
    "                loss_value = sess.run(loss, feed_dict= \\\n",
    "                    {inputIm_placeholder: batch_images_gen, \\\n",
    "                     question_placeholder: batch_questions_gen, \\\n",
    "                     answer_placeholder: batch_answers_gen, \\\n",
    "                     option1_placeholder: batch_o1, \\\n",
    "                     option2_placeholder: batch_o2, \\\n",
    "                     option3_placeholder: batch_o3, \\\n",
    "                     labels_placeholder: labels\n",
    "                     })\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                v_batch_generator = generator(val_samples, cfg.batch_size)\n",
    "                v_batch_images_gen, v_batch_questions_gen, v_batch_answers_gen, v_batch_o1, v_batch_o2, \\\n",
    "                v_batch_o3, v_labels = v_batch_generator.__next__()\n",
    "\n",
    "                accuracy_value = sess.run(accuracy, feed_dict= \\\n",
    "                    {inputIm_placeholder: v_batch_images_gen, \\\n",
    "                     question_placeholder: v_batch_questions_gen, \\\n",
    "                     answer_placeholder: v_batch_answers_gen, \\\n",
    "                     option1_placeholder: v_batch_o1, \\\n",
    "                     option2_placeholder: v_batch_o2, \\\n",
    "                     option3_placeholder: v_batch_o3, \\\n",
    "                     labels_placeholder: v_labels\n",
    "                     })\n",
    "                print(\"Iter: \", j, ' Total iter: ', total_iterations, \" Loss value is: \", loss_value, \" Time taken: \",\n",
    "                      end_time - start_time, \" Accuracy value is : \", accuracy_value / float(cfg.batch_size))\n",
    "\n",
    "    saver.save(sess, savefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
